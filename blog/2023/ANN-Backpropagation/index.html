<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ANN Backpropagation Process | Yuanlai He (Miles) </title> <meta name="author" content="Miles (Yuanlai) Ho"> <meta name="description" content="About ANN Backpropagation Process"> <meta name="keywords" content="Software Development Engineer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/command-line.png?0c818eda30e553ba2f0d187ca37db021"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://imilesho.github.io/blog/2023/ANN-Backpropagation/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yuanlai He (Miles) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ANN Backpropagation Process</h1> <p class="post-meta"> November 01, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/mechinelearning"> <i class="fa-solid fa-hashtag fa-sm"></i> MechineLearning</a>     ·   <a href="/blog/category/posts"> <i class="fa-solid fa-tag fa-sm"></i> posts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Backpropagation is a supervised learning algorithm used for training artificial neural networks (ANNs), including multilayer perceptrons (MLPs). It employs the chain rule of calculus to efficiently compute gradients of the loss function with respect to the weights of the network, which are then used to update the weights.</p> <p>Here’s a simplified breakdown of how backpropagation works in a neural network:</p> <h3 id="1-forward-pass">1. <strong>Forward Pass:</strong> </h3> <ul> <li>Input is passed through the network.</li> <li>Each layer applies weights, biases and activation functions.</li> <li>The final output is compared with the target using a loss function to calculate the error.</li> </ul> <h3 id="2-backward-pass-backpropagation">2. <strong>Backward Pass (Backpropagation):</strong> </h3> <ul> <li>Calculate the gradient of the loss function with respect to each weight by propagating the gradient backward through the network from the output layer to the input layer.</li> <li>Update the weights using a method such as gradient descent.</li> </ul> <h3 id="example-simplified-calculation-process-in-a-mlp">Example: Simplified Calculation Process in a MLP</h3> <p>Let’s consider a simple neural network with one hidden layer. Suppose we have:</p> <ul> <li>Input (x)</li> <li>Weights from input to hidden layer (w_1)</li> <li>Weights from hidden to output layer (w_2)</li> <li>Activation functions (f) for the hidden layer and (g) for the output layer</li> </ul> <h4 id="forward-pass">Forward Pass:</h4> <ol> <li>Calculate net input to the hidden layer: [ z_1 = w_1 \cdot x + b_1 ]</li> <li>Apply activation function: [ h = f(z_1) ]</li> <li>Calculate net input to the output layer: [ z_2 = w_2 \cdot h + b_2 ]</li> <li>Apply activation function to get the final output: [ y = g(z_2) ]</li> </ol> <h4 id="backward-pass">Backward Pass:</h4> <ol> <li>Calculate the derivative of the loss function (L) with respect to the output: [ \frac{\partial L}{\partial y} = 2(y - \text{target}) ]</li> <li>Calculate the gradient with respect to the weights (w_2) between hidden and output layers: [ \frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_2} = \frac{\partial L}{\partial y} \cdot g’(z_2) \cdot h ]</li> <li>Calculate the gradient with respect to the weights (w_1) between input and hidden layers: [ \frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_2} \cdot \frac{\partial z_2}{\partial h} \cdot \frac{\partial h}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1} = \frac{\partial L}{\partial y} \cdot g’(z_2) \cdot w_2 \cdot f’(z_1) \cdot x ]</li> <li> <p>Update the weights (w_1) and (w_2) using the calculated gradients, typically using a method like gradient descent: [ w_1 = w_1 - \eta \frac{\partial L}{\partial w_1} ] [ w_2 = w_2 - \eta \frac{\partial L}{\partial w_2} ]</p> <p>Here, (\eta) is the learning rate, a hyperparameter you choose to control the size of the steps taken during optimization.</p> </li> <li> <p>The gradient with respect to the output layer bias, (b_2), is: [ \frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_2} = \frac{\partial L}{\partial y} \cdot g’(z_2) ]</p> </li> <li> <p>The gradient with respect to the hidden layer bias, (b_1), is: [ \frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_2} \cdot \frac{\partial z_2}{\partial h} \cdot \frac{\partial h}{\partial z_1} = \frac{\partial L}{\partial y} \cdot g’(z_2) \cdot w_2 \cdot f’(z_1) ]</p> </li> <li>Update the biases using the gradients: [ b_1 = b_1 - \eta \frac{\partial L}{\partial b_1} ] [ b_2 = b_2 - \eta \frac{\partial L}{\partial b_2} ]</li> </ol> <p>Note that when you’re calculating the gradients with respect to the biases, you’re essentially asking, “How does the loss change as I adjust the bias?” The reason you don’t see (x) or (h) in the gradient formulas for biases is that the derivative of a bias (being an additive term) with respect to itself is 1.</p> <p>Including biases in the neural network allows the network to better fit the data by allowing flexibility in adjusting the activations. Without biases, the neuron’s output would always be forced to go through the origin (0,0) in the input-output space, which would greatly limit the representational power of the network.</p> <p>In this way, by repeatedly applying the backpropagation algorithm, the neural network learns to adjust its weights to minimize the error between its predictions and the actual target values, thus training the model. Note that this example is greatly simplified for illustration; real-world neural networks typically involve many more layers and neurons, and various additional considerations such as batch processing, regularization, and various optimization algorithms.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/LinearAlgebraFoundation/">Linear Algebra Foundation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/deeplearning-activaiton-functions/">The Activation Functions in Deep Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/adaboost/">Adaboost (Adaptive Boosting) Homework</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/the-shell/">The Shell Command Line and File System Basics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/IAM-and-IAM-Identity-Center/">IAM and IAM Identity Center</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"iMilesHo/iMilesHo.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Miles (Yuanlai) Ho. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>